{
  "source_url": "https://ai.pydantic.dev",
  "instructions": "scrape anything related to RAG",
  "summary": "PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Pydantic is the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more. Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral. PydanticAI is in early beta, the API is still subject to change and there's a lot more to do. Here is a minimal example using Pydantic AI to build a support agent for a bank. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Dynamic system prompts can be registered with the @agent.system_prompt decorator, and can make use of dependency injections. Pydantic is used to validate arguments, and errors are passed back to the LLM so it can retry. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Pydantic's FastAPI revolutionized web development by offering an ergonomic design, built on the foundation of Pydicantic. PydanticAI is a Python-centric tool for building AI-driven apps. Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more) Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for other models. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Static system prompts can be registered with the system_prompt keyword argument to the agent. Dynamic system prompts and tool functions can make use of dependency injections. Here we configure the agent to use OpenAI's GPT-4o model. PydanticAI is an open-source, RESTful API for Pydantic Logfire. It can be used to run an agent asynchronously to provide customer support in a bank. The agent will exchange multiple messages with the LLM as tools are called to retrieve a result. The response from the agent will, be guaranteed to be a SupportResult , if validation fails reflection will mean the agent is prompted to try again. Pydantic Services Inc. 2024 to present Agents - PydanticAI. Running Agents Iterating Over an Agent's Graph. Using .next(...) manually Accessing usage and the final result. Streaming Additional Configuration Usage Limits Model (Run) Settings Model specific settings Runs vs. Conversations Type safe by design System Prompts Reflection and self-correction Model errors. PydanticAI is a library for building and running finite state machines in Python. Agents are intended to be instantiated once (frequently as module globals) and reused throughout your application. There are four ways to run an agent: agent.run() a coroutine which returns a RunResult containing a completed response. agent.iter() a context manager which returns an AgentRun over the nodes of the agent's underlying Graph. PydanticAI exposes the lower-level iteration process via Agent.iter . This method returns an AgentRun , which you can async-iterate over. You can also drive the iteration manually by passing the node you want to run next to the AgentRun. You can retrieve usage statistics (tokens, requests, etc.) at any time from the AgentRun object via agent_run. usage() . This method returns a Usage object containing the usage data. When you call await agent_ run.next(node) , it executes that node in the agent's graph, updates the run's history, and returns the next node to run. PydanticAI offers a UsageLimits structure to help you limit your usage (tokens and/or requests) on model runs. Consider the following example, where we limit the number of response tokens from pydantic_exceptions. PydanticAI offers a settings.ModelSettings structure to help you fine tune your requests. This structure allows you to configure common parameters that influence the model's behavior, such as temperature, max_tokens, timeout , and more. An agent run might represent an entire conversation. There's no limit to how many messages can be exchanged in a single run. A conversation might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls.  system prompts are known when writing the code and can be defined via the system_prompt parameter of the Agent constructor. Dynamic system prompts depend in some way on context that isn't known until runtime. You can add both to a single agent; they're appended in the order they're defined at runtime. The default retry count is 1 but can be altered for the entire agent , a specific tool , or a result validator. validation can be passed back to the model with a request to retry. If models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns 503), agent runs will raise UnexpectedModelBehavior. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more). Model-agnostic : Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral. Pydantica Logfire Integration : Seamlessly integrates with Pydanic Logfire for real-time debugging, performance monitoring, and behavior tracking. Pydantic Graph provides a powerful way to define graphs using typing hints. Streamed Responses provides the ability to stream LLM outputs continuously. In Beta PydanticAI is in early beta, the API is still subject to change and there's a lot more to do. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Dynamic system prompts can be registered with the @agent.system_prompt decorator. The SupportDependencies dataclass is used to pass data, connections and logic into the model that will be needed when running system prompt and tool functions. PydanticAI is a Pydantic model that constrain the structured data returned by the agent. The model tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run. You can watch the agent in action by setting up logfire and adding the following code. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more). Model-agnostic Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral. PydanticAI is an open-source programming language. Pydantic can be used to build agents, tools, and dynamic system prompts. We can use dependency injection and structured responses to build more powerful agents. PydanticAI is an artificial intelligence tool for the Python programming language. It can be used to build applications using Pydantic and other Python libraries. The tool is written in Python and requires Python 3.9 or higher. Pydantic is a free, open-source Python distribution. It's available on GitHub and the Python 3 operating system. The release is signed by the Pydantic Foundation and is available for download from the GitHub repository. Pydantic is a free, open-source, cloud-based chat app for iOS and Android. Pydantic has a Graph API that lets you test and monitor your messages and chat history. It also has a Pingdom monitoring tool to help you monitor your data.  255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 \"Class for defining \"agents\" - a way to have a specific type of \"conversation\" with anversation. \"Data\" is a way of defining the type of data to be collected. \"Dataclasses\" are a type of dataclasses that can be used to collect data. Agent is generic in the dependency type they take and the result data type they return. By default, if neither generic parameter is customised, agents have type `Agent[None, str]`. Minimal usage example: print(result.data) #> Paris. LLM. Model is the default model to use for this agent, if not provide, you must provide the model when calling it. deps_type: The type used for dependency injection. name: The name of the agent, used for logging. tools: Tools to register with the agent. instrument: Automatically instrument with OpenTelemetry. Will use Logfire if it's configured. end_strategy: Strategy for handling tool calls. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an `AgentRun` object. For more details, see the documentation of `Agent run. Model is an optional model to use for this run, required if `model` was not set when creating the agent. deps are Optional dependencies to use. usage_limits are Optional limits on model request count or token usage. message_history is History of the conversation so far. infer_name is Whether to try to infer the agent name from the call frame if it's not set. @overload def run_sync ( self , user_prompt : str | Sequence [ _messages . UserContent ]): ... @overloaddef agent = Agent('openai:gpt-4o') result_sync = agent.run_sync('What is the capital of Italy?') print(result_sync.data) #> Rome. Run the agent with a user prompt in async mode, returning a streamed response. Args: user_prompt: User input to start/continue the conversation. infer_name: Whether to try to infer the agent name from the call frame if it's not set. usage: Optional usage to start with, useful for resuming a conversation or agents used in tools. Model is an optional model to use for this run, required if `model` was not set when creating the agent. deps is a list of optional dependencies for the run. usage is an Optional usage to start with, useful for resuming a conversation or agents used in tools. infer_name is whether to try to infer the agent name from the call frame. The override function is used to temporarily override agent dependencies and model. This is particularly useful when testing. You can find an example of this [here]:\u00a0overriding-model-via-pytest-fixtures. Decorator to register a system prompt function. Can decorate a sync or async function. Decorator can be used either bare (`agent.system_prompt`) or as a function call (` agent.system-prompt(...)`) The tool decorator function is used to decorate a sync or async function. It takes the function definition as its first argument. It can be used to customise a tool at call time, or omit it completely from a step. The docstring is inspected to extract both the tool description and description of each parameter. The tool function decorator is used to decorate a sync or async function. It can only decorate functions that don't take the RunContext as an argument. The tool format is inferred from the structure of the docstring. The PyTypeChecker tool function is used to get the agent name from the call frame. It is also used to check that the model has been set before calling the agent. The agent name is passed to the deps function, which is then used to determine which deps to use. The default model configured for this agent is Model. The model instance-attribute is knownModelName. The last_run_messages attribute has been removed, use 'capture_run-messages' instead. The final result is a Graph. Pydantic is a free, open-source logging framework. It's used by OpenTelemetry and Logfire. The agent is created with the following parameters: name, type, tools, model, retries, tools and instrument. Model is the default model to use for this agent, if not provide, you must provide the model when calling it. deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully parameterize the agent. tools: Tools to register with the agent, you can also register tools via the decorators. instrument: Automatically instrument with OpenTelemetry. Will use Logfire if it's configured. end_strategy: Strategy for handling tool calls that are requested alongside a final result. The name of the agent, used for logging. If None , we try to infer the agent name from the call frame when the agent is first run. Automatically instrument with OpenTelemetry. Will use Logfire if it's configured. If model_settings is provided by run , run_sync , or run_stream , those settings will be merged with this value. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Run the agent with a user prompt in async mode. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an AgentRun object. A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. AgentRun is a tool that allows you to access the full message history, new messages, and usage statistics. For more details, see the documentation of AgentRun . Example: from pydantic_ai import Agent agent = Agent ( 'openai:gpt-4o' ) async def main (): nodes = [] with agent . iter ( 'What is the capital of France?' ) as agent_run : async for node in agent_ run : nodes . append ( node ) print ( nodes ) ''' [ ModelRequest node( request=ModelRequest( parts=[ UserPromptPart( content='What isTheCapital of France?', timestamp=datetime.datetime(...), part_kind='user 'AgentRun' is a contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an `AgentRun` object. The `Agent run' API is also used to consume the outputs coming from each LLM model response. We consider it a user error if a user tries to restrict the result type while having a result validator. Instead of this, copy the function tools to ensure they don't share current_retry state between agent # runs. This is a convenience method that wraps self.run with loop. You therefore can't use this method inside async code or if there's an active event loop. It syncs the agent with a user prompt to start/continue the conversation. Synchronously run the agent with a user prompt. This is a convenience method that wraps [`self.run`][pydantic_ai.Agent.run] with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop. Run the agent with a user prompt in async mode, returning a streamed response. Example: from pydantic_ai import Agent agent = Agent ( 'openai:gpt-4o' ) async def main (): async with agent . run_stream ( 'What is the capital of the UK?' ) as response : print ( await response . get_data ()) #> London. Run the agent with a user prompt in async mode, returning a streamed response. Example: 'What is the capital of the UK?' will return 'London' as the result. The 'iter' method will deprecate now that we have the `iter` method. We need an event for when we reach the final result. The on_complete function is called when the stream has completed. It is used by the context manager to temporarily override agent dependencies and model. This is particularly useful when testing. You can find an example of this here. Decorator to register a system prompt function. Optionally takes RunContext as its only argument. Can decorate a sync or async function. Decorator can be used either bare ( agent.system_prompt ) or as a function call. The decorator can be used either bare (`agent.system_prompt`) or as a function call (` agent.system-prompt(...)`), see the examples below. Decorator to register a result validator function. Optionally takes RunContext as its first argument. Decorator to register a result validator function. Optionally takes [`RunContext`] as its first argument. Overloads for every possible signature of `result_validator` are included so the decorator doesn't obscure the type of function. Pydantic_ai's tool function decorator can be used to decorate a sync or async function. It takes a docstring to extract both the tool description and description of each parameter. It can't add overloads for every possible signature of tool. PyTypeChecker allows you to inspect the docstring to extract the tool description and description of each parameter. The number of retries to allow for this tool, defaults to the agent's default retries, which defaults to 1. If True, raise an error if a parameter description is missing. Defaults to False. The tool_plain function decorates a function which doesn't take an argument as an argument. The docstring is inspected to extract both the tool description and description of each parameter. The return type is a recursive union so the signature of functions decorated with `@agent.tool` is obscured. A stateful, async-iterable run of an Agent. You generally obtain an AgentRun instance by calling my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. You can also manually drive the iteration using the next method for more granular control. Source code in pydantic_ai/agent.py 1223 1224 1225 1226 1227 1228 1229 1230 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 12 @property def ctx ( self ) is the current context of the agent run. @property def next_node is the next node that will be run in the agent graph. @ property def result is the final result of the run if it has ended. The agent class provides an interface for iteration over the nodes in the agent run. It also provides usage statistics for the run so far, including token usage, model requests, and so on. The final result is the result of the run if it has ended, otherwise None. Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an End node. Nodes are the nodes to run next in the graph. The final result of an agent run is the result of the run's usage statistics. The graph logic returns the next node returned by the graph logic, or an [`End`] node if the run has completed.  def all_messages ( self , * , result_tool_return_content : str | None = None ) : \"\"\"Return the history of _messages. tool return content when the return type is `str`.''') def new_message ( self, *, result_ ToolReturnPart : isinstance ( part , _Messages . ToolReturn part )): \"\"\"Return new messages associated with this run. Messages from older runs are excluded. Returns: JSON bytes representing the new messages. The usage function returns the usage of the whole run. The all_messages function returns a list of all the messages in the current session. The new message function returns new messages associated with the current message. Pydantic_ai_slim/pydantic-ai/agent.py provides a convenient way to modify the content of the result tool call if you want to continue the conversation. If None , the last message will not be modified. PydanticAI ships with native tools that can be used to enhance your agent's capabilities. DuckDuckGo search tool allows you to search the web for information. Pydantic Model Weather agent Bank support SQL Generation Flight booking RAG Stream markdown Stream whales Chat App with FastAPI Question Graph API Reference API Reference. There isn't yet a single, universally accepted list with final numbers that names the complete top five. Based on what several sources note so far, the two undisputed leaders are: 1. Ne Zha 2 2. Inside Out 2 3\u00e2\u0080\u00935. Other animated films (yet to be definitively finalized across all reporting outlets) If you're looking for a final, consensus list of the top five, it may be best to wait until the 2025 year\u00e2\u0081\u0091end box\u00e2\u0089office tallies are in. Pydantic Services Inc. 2024 to present Chat App with FastAPI - PydanticAI. Showing documentation for the latest release v0.0.32 2025-03-04. Simple chat app example build with Fast API. Demonstrates: reusing chat history serializing messages. @app . get ( /chat_app.ts) @app . post ( '/chat/) async def post_chat ( prompt : Annotated [ str , fastapi . Form ()], database : Database = Depends ( get_db ) ) -> StreamingResponse : async. FileResponse : return FileResponse ( media_type = 'text/plain' ) @app. get (/chat/' ) async. Class ChatMessage ( TypedDict): \"\"\"Format of messages sent to the browser.\"\"\" role : Literal [ 'user' , 'model' ] timestamp : str content : str The chat app uses a simple database to store messages. The standard library package is synchronous, so we use a thread pool executor to run queries asynchronously. A simple HTML page to render the app: chat_app.html <!DOCTYPE html> The chat app uses TypeScript to handle rendering the messages. The code is passed to the browser as plain text and transpiled in the browser. To make this simple, we use a dirty, non-production-ready hack to transpile the TypeScript code.  browser. chat_app.ts // BIG FAT WARNING: to avoid the complexity of npm, this typescript is compiled in the browser // there's currently no static type checking import { marked } from 'https://cdnjs.cloudflare.com/ajax/libs/marked/15.0.0/lib/marked.esm' const convElement = document . getElementById ( 'conversation' ) const promptInput = 'prompt-input' as HTMLInputElement const spinner = 'spinner' // stream the response and render messages as each chunk is received // data is sent as newline-delimited JSON async function onFetchResponse ( response : Response ) : Agents are PydanticAI's primary interface for interacting with LLMs. In some use cases a single Agent will control an entire application or component. Multiple agents can also interact to embody more complex workflows. The Agent class has full API documentation. Pydantic AI is a free, open-source tool for building and running agents. Agents are designed to be instantiated once and reused throughout your application. There are four ways to run an agent: agent.run() a coroutine which returns a RunResult containing a completed response. agent. run_sync() a plain, synchronous function which returns the completed response as an async iterable. Each Agent in PydanticAI uses pydantic-graph to manage its execution flow. You can pass messages from previous runs to continue a conversation or provide context. The AgentRun is anync-iterable that yields each node ( Base or End) in the flow. The agent run is finished once an End node has been produced. When you call await agent_run.next(node) , it executes that node in the agent's graph, updates the run's history, and returns the next node to run. You can retrieve usage statistics (tokens, requests, etc.) at any time from the AgentRun object via agent_ run. usage() . This method returns a Usage object containing the usage data. The code is written as a node-by- node, streaming iteration. The code is called weather_agent. ( location , forecast_date ) output_messages : list [ str ] = [] async def main (): user_prompt = 'What will the weather be like in Paris on Tuesday?' as run: async with node . stream ( run . ctx ) as request_stream: async for event in request_ stream: if isinstance ( event , PartStartEvent): output_Messages . append ( f '[Request] Starting part { event . index } : { event ! part !r} ' ) elif isinstance  ( event . delta , TextPartDelta): output-messages . PydanticAI offers a UsageLimits structure to help you limit your usage (tokens and/or requests) on model runs. You can apply these settings by passing the usage_limits argument to the run{_sync,_stream} functions. Restricting the number of requests can be useful in preventing infinite loops or excessive tool calling. PydanticAI is an open-source, static type checker. It's designed to work well with static types like mypy and pyright. Pydantic AI can be used to create conversations between agents. PydanticAI uses Pydantic, which uses type hints as the definition for schema and validation. Some types (specifically type hints on parameters to tools, and the result_type arguments to Agent ) are used at runtime. We (the library developers) have messed up if type hints are confusing you more than helping you, please create an issue explaining what's annoying you! Static system prompt is defined at agent creation time. Dynamic system prompt defined via a decorator with RunContext. Validation errors from both function tool parameter validation and structured result validation can be passed back to the model with a request to retry. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Pydantic Logfire. It is built by the same team behind the GenAPI app development layer used by the Pydantica Weather agent. It uses the same validation layer as the one used by Pydatory Logfire, so it is easy to test and validate. PydanticAI is a Python-centric tool for building AI-driven projects. It uses Pydantic Logfire for real-time debugging, performance monitoring, and behavior tracking. The API is in early beta, the API is still subject to change and there's a lot more to do. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Dynamic system prompts can be registered with the @agent.system_prompt decorator. The SupportDependencies dataclass is used to pass data, connections and logic into the model that will be needed when running system prompt and tool functions. PydanticAI is an open-source, RESTful API for building customer support agents. It can be used to connect to an external database to get information about customers. The Pydantic model is used to constrain the structured data returned by the agent. It also performs validation to guarantee the data is correct. Pydantic is a free, open-source, cloud-based chat tool. It's used by Google, Facebook, Twitter, and others. Pydantic can be used to create, test, and monitor multi-agent applications. It can also be used as a tool to help you with your own applications. Model is an ABC abstract class for a model. It can be used to make a request to a model and return a streaming response. The request method is not required, but you need to implement it if you want to support streamed responses. Streamed response from an LLM when calling a tool. This method should be implemented by subclasses to translate vendor-specific stream of events into pydantic_ai-format events. It should use the `_parts_manager` to handle deltas. AllOW_MODEL_REQUESTS is a global setting that allows you to disable request to most models. The testing models TestModel and FunctionModel are no affected by this setting. If you're defining your own models that have costs or latency associated with their use, you should call this in Modelrequest and Modelrequest_stream. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more) Model-agnostic: Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral. Streamed Responses: Provides the ability to stream LLM outputs continuously, with immediate validation, ensuring rapid and accurate results. Use PydanticAI to build a support agent for a bank. Run the agent synchronously, conducting a conversation with the LLM. The agent will act as first-tier support in the bank. We can easily add \"tools\" and dynamic system prompts. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Dynamic system prompts can be registered with the @agent.system_prompt decorator. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry. PydanticAI is a free, open-source, cloud-based tool for building and managing agents. Agents are Pydantic's primary interface for interacting with LLMs. In some use cases a single Agent will control an entire application or component, but multiple agents can also interact to embody more complex workflows. There are four ways to run an agent: agent.run() a coroutine which returns a RunResult containing a completed response. agent. run_sync() is a plain, synchronous function which returns an AgentRun over the nodes of the agent's underlying Graph. Agents are designed for reuse, like FastAPI Apps. Each Agent in PydanticAI uses pydantic-graph to manage its execution flow. The AgentRun method returns an AgentRun , which you can async-iterate over. You can also drive the iteration manually by passing the node you want to run next. Streaming an agent run in combination with async for iteration. You can retrieve usage statistics (tokens, requests, etc.) at any time from the AgentRun object via agent_run. usage() . This method returns a Usage object containing the usage data. Weather service is written as a node-by- node, streaming iteration. Weather service can be used to stream data from a user prompt or a model request node. It can also be used as a handle-response node to send data back to the model. The model can then use this data to call a tool. PydanticAI offers a UsageLimits structure to help you limit your usage (tokens and/or requests) on model runs. You can apply these settings by passing the usage_limits argument to the run{_sync,_stream} functions. Restricting the number of requests can be useful in preventing infinite loops. PydanticAI is designed to work well with static type checkers, like mypy and pyright. Typing is (somewhat) optional for you if you choose to use it. Pydantic uses type hints as the definition for schema and validation.  type hints are used to ensure you're using the right types. Agents are generic in both the type of their dependencies and the types they return. Consider the following script with type mistakes: type_mistakes.py. The agent is defined as expecting an instance of User as deps. But here add_user_name is defined to take a str as the dependency.  static system prompt defined at agent creation time. Dynamic system prompt called just after run_sync. Validation errors can be passed back to the model with a request to retry. The default retry count is 1 but can be altered for the entire agent. The Pydantic API is free and open source. Use it to create, test, and manage your own apps. Use the API to test and monitor your apps and services. The API is open source and can be downloaded from GitHub. The structure of ModelMessage can be shown as a graph: graph RL SystemPromptPart. Source code in pydantic_ai_slim/pydantic-ai/messages.py. v0.0.32 2025-03-04 . pydantacai.messages Binary content is binary content, e.g. an audio or image file. A user prompt is written by the end user. Content comes from the user_prompt parameter of Agent.run. Gemini has a built-in tool return part that can be used to send a message to a model. The tool return message is a string representation of the result of running a tool. It can also be used as a tool call identifier. A message back to a model asking it to try again. This can be sent for a number of reasons: Pydantic validation of tool arguments failed. A tool raised a ModelRetry exception. The model returned plain text when a structured response was expected.  content is derived from a Pydantic [`ValidationError`] * a result validator raised a [`ModelRetry`] exception. If the retry was triggered by a ValidationError , this will be a list of error details. ModelRequest is a request generated by PydanticAI and sent to a model. TextPart is a plain text response from a model and ToolCallPart is an optional tool call. The arguments to pass to the tool are stored as a string or a Python dictionary. args instance-attributeargs : str | dict [ str , Any ] The arguments to pass to the tool. This is stored either as a JSON string or a Python dictionary depending on how data was received. tool_name instance- attribute tool_ name : str The name of the tool to call. part_kind class-attribute part_ kind : Literal [ 'tool-call' ] = 'tool' PydanticAI is a free, open-source AI app. It allows you to send messages to the Pydantic AI app from a web browser. The messages can be sent to the app by any of a number of methods. Otel events return OpenTelemetry events for the response. If the model provides a timestamp in the response (as OpenAI does) that will be used. TextPartDelta is a partial update (delta) for a TextPart to append new content. Applying a text delta to an existing TextPart raises a ValueError if part is not a TextPart. ToolCallPartDelta is a partial update (delta) for a ToolCall part to modify tool name, arguments, or tool call ID. @overload def apply ( self , part : ModelResponsePart ) -> ToolCallPart : ... @overloaddef apply (self, part : Tool callPartDelta) -> ToolcallPart | ToolCall partDelta: ... def _apply_to_delta ( self, delta : ToolCallCallPartDelta): \"\"\"Internal helper to apply this delta to another delta.\"\"\" if self . tool_call_id : # Set the tool_ call_id if it wasn't present, otherwise error if it has changed. None return Tool call part ( self .tool_call-id , self . tools_name-delta ) Part delta type identifier, used as a discriminator. Optional tool call identifier, this is used by some models including OpenAI. Incremental text to add to the existing tool name, if any. If this is a string, it will be appended to existing JSON arguments. If it is a dict, it'll be merged with existing dict arguments.  apply ( part : ModelResponsePart | ToolCallPartDelta , ) returns a new part or delta with the changes applied. A partial update (delta) for any model response part. If multiple PartStartEvent s are received with the same index, the new one should fully replace the old one. An event in the model response stream, either starting a new part or applying a delta to an existing one. An event indicating the response to the current model request matches the result schema. A tool call ID, if any, that this result is associated with. PydanticAI Agents are Pydantic's primary interface for interacting with LLMs. In some use cases a single Agent will control an entire application or component. Multiple agents can also interact to embody more complex workflows. The Agent class has full API documentation. There are four ways to run an agent. Agents are designed for reuse, like FastAPI Apps or APIRouter apps. Agents can be instantiated once (frequently as module globals) and reused throughout your application. Each Agent in PydanticAI uses pydantic-graph to manage its execution flow. The AgentRun method returns an AgentRun , which you can async-iterate over. You can also drive the iteration manually by passing the node you want to run next to the Agent run. The agent run is finished once an End node has been produced. When you call await agent_run.next(node) , it executes that node in the agent's graph, updates the run's history, and returns the next node to run. You can retrieve usage statistics (tokens, requests, etc.) at any time from the AgentRun object. Start a node-by- node, streaming iteration with weather_agent . iter ( user_prompt , deps = WeatherService ()) as run. If the user has provided input, stream tokens from the model's request node. The model returned some data, potentially calls a tool. PydanticAI offers a UsageLimits structure to help you limit your usage (tokens and/or requests) on model runs. Restricting the number of requests can be useful in preventing infinite loops or excessive tool calling. The settings.ModelSettings structure allows you to configure common parameters that influence the model's behavior. PydanticAI is designed to work well with static type checkers, like mypy and pyright. Typing is (somewhat) optional to make type checking as useful as possible for you. Pydantic uses type hints as the definition for schema and validation. The type hints are used to ensure you're using the right types. Consider the following script with type mistakes: type_mistakes.py. Static system prompts are known when writing the code. Dynamic system prompts depend in some way on context that isn't known until runtime.  static system prompt defined at agent creation time. Dynamic system prompt called just after run_sync. Validation errors from both function tool parameter validation and structured result validation can be passed back to the model with a request to retry. PydanticAI is a Python agent framework designed to make it less painful to build applications with Generative AI. Pydantic is built on the foundation of LLM, virtually every agent framework and library in Python uses Pydant. PydanticAI is a Python-centric tool for building AI-driven projects. It integrates with Pydantic Logfire for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications. The API is still subject to change and there's a lot more to do. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Static system prompts can be registered with the system_prompt keyword argument to the agent. Dynamic system Prompts can also be registered using the @agent.system_ Prompt decorator. PydanticAI is a free, open-source, Pydantic-powered agent orchestration tool. It can be used to connect to external databases to get information about customers. The agent will run asynchronously, exchanging multiple messages with the LLM. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. The API is in early beta, the API is still subject to change and there's a lot more to do. Use PydanticAI to build a support agent for a bank. The agent will send the system prompt and the user query to the LLM. The model will return a text response. We can easily add \"tools\", dynamic system prompts, and structured responses. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Dynamic system prompts can be registered with the @agent.system_prompt decorator. Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation. Using Pydantic Logfire, we can watch the agent in action using the examples below. In our demo, DatabaseConn uses asyncpg to connect to a Postgres database. See Monitoring and Performance to learn more. Read the API Reference to understand PydantAI's interface. Agent delegation doesn't need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final result. of the run will not be possible, but you can still use UsageLimits to avoid unexpected costs. (This example is complete, it can be run \"as is\") \"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession. Here we show two agents used in succession, the first to find a flight and the second to extract the user's seat preference. This example shows how even a fairly simple agent delegation can lead to a complex control flow.  class SeatPreference ( BaseModel): row : int = Field ( ge = 1 , le = 30 ) seat : Literal [ 'A' , 'B' , \u2018C\u2019 , 'D\u2019, \u2018E\u2019 and \u2018F\u2019 ] # This agent is responsible for extracting the user\u2019s seat selection seat_preference_agent = Agent [ None , Union [ Seat preference , Failed ]] # In reality, this would call a flight search API or use a browser to scrape a flightSearch website. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. It offers an innovative and ergonomic design, built with FastAPI. The following examples demonstrate how to use dependencies. PydanticAI is a Python tool for building AI apps. Built by the team behind Pydantic, the validation layer of the OpenAI SDK. Model-agnostic: Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral. Type-safe: Designed to make type checking as powerful and informative as possible. Streamed Responses: Provides the ability to stream LLM outputs continuously. PydanticAI's system of dependency injection provides a type-safe way to customise the behavior of your agents. Dynamic system prompts can be registered with the @system_prompt decorator, and can make use of dependency injections. Here we configure the agent to use OpenAI's GPT-4o model, you can also set the model when running the agent. Pydantic Logfire lets you run an agent asynchronously. The agent will exchange messages with the LLM as tools are called to retrieve a result. The result will be validated with Pydantic to guarantee it is a SupportResult. PydanticAI is a free, open-source, cross-platform tool for building applications with agents. Use the API Reference to understand Pydantic AI's interface. The Agent class has full API documentation, but conceptually you can think of an agent as a container for: Component Description System prompt(s) A set of instructions for the LLM written by the developer. Structured result type The structured datatype the LLm must return at the end of a run. There are four ways to run an agent: agent.run() is a coroutine which returns a RunResult. agent. run_stream() returns a StreamedRunResult which contains methods to stream a response as an async iterable. agent .iter() returns an AgentRun over the nodes of the agent's underlying Graph. Agents are designed for reuse, like FastAPI Apps.  pydantic-graph is a generic, type-centric library for building and running finite state machines in Python. PydanticAI makes use of it to orchestrate the handling of model requests and model responses in an agent's run. You can retrieve usage statistics (tokens, requests, etc.) at any time from the AgentRun object via agent_run. usage() . This method returns a Usage object containing the usage data. Once the run finishes, agent_ run.final_result becomes a AgentRun result containing the final output. Here is an example of streaming an agent run in combination with async for iteration. PydanticAI offers a UsageLimits structure to help you limit your usage (tens and/or requests) on model runs. You can use it to limit the number of requests you make to a given model. PydanticAI offers a settings.ModelSettings structure to help you fine tune your requests. This structure allows you to configure common parameters that influence the model's behavior. can apply these settings by passing the usage_limits argument to the run{_sync,_stream} functions. Restricting the number of requests can be useful in preventing infinite loops or excessive tool calling. PydanticAI is designed to work well with static type checkers, like mypy and pyright. Typing is (somewhat) optional to make type checking as useful as possible for you. Pydantic uses type hints as the definition for schema and validation. System prompts fall into two categories: Static system prompts defined at agent creation time. Dynamic system prompts depend in some way on context that isn't known until runtime. You can add both to a single agent; they're appended in the order they're defined at runtime. You can raise ModelRetry from within a tool or result validator function to tell the model it should retry generating a response. The default retry count is 1 but can be altered for the entire agent or a specific tool. Validation errors from both function tool parameter validation and structured result validation can be passed back to the model with a request to retry. PydanticAI is a free, open-source, AI-powered chat app. The latest version of Pydantic is v0.0.32. The toolkit is available on GitHub. Use this guide to help you install and use the tool. Dice game is a fun way to test your knowledge of a game. Let's run the game and see what happens. We use the Gemini flash model to test the game. We pass the user's name as the dependency. We then print the messages from that game. DiceGame uses function tools to let the model know what is available to call. Tools or functions are also used to define the schema(s) for structured responses. PydanticAI extracts the docstring from functions and (thanks to griffe) extracts parameter descriptions from thedocstring and adds them to the schema. Pydantic is a free, open-source Python programming language. It's used to create and test Dynamic Function tools. The tool definition can be customised at each step of a run. Here's a simple prepare method that only includes the tool if the value of the dependency is 42 . As with the previous example, we use TestModel to demonstrate the behavior without calling a real model. Here's a more complex example where we change the description of the name parameter to based on the value on deps. PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Pydantic is the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, and many more. PydanticAI is a Python-centric tool for building AI-driven projects. Uses Pydantic Graph to define graphs using typing hints. Streamed Responses provides the ability to stream LLM outputs continuously, with immediate validation. Dependency Injection System provides an optional dependency injection system to provide data and services to your agent's system prompts and tools. The agent is generic in the type of dependencies it accepts and the result it returns. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry. Dynamic system prompts can make use of dependency injection. PydanticAI is a Pydantic Agent Framework shim for LLMs ai.pydantic.dev. The API Reference is a good place to start building your own applications. Use the examples to learn more about building applications with PydantAI.  74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 6 Pydantic_ai is an open-source logging tool. It can be used to provide a specific type of \"conversation\" with an LLM. There are a number of built-in features that make it easy to use. The agent is created with a default model and a default system prompt. It also has a tool and a tool description. The tool description is the description of the final result tool. There are also tools to register with the agent, which can be done via decorators. The agent can also be used for logging. Set this to `false` to defer the evaluation until the first run. instrument: Automatically instrument with OpenTelemetry. Will use Logfire if it's configured. end_strategy: Strategy for handling tool calls that are requested alongside a final result. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned.  can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics. Model.settings is an object with a model and an instrumented model. Model.settings means that the model is set to the same settings as the InstrumentedModel. Tracer is a function that takes a number of parameters and returns a tracer if it is notinstance. Synchronously run the agent with a user prompt. This is a convenience method that wraps [`self.run`] with 'loop.run_until_complete(...)'. You therefore can't use this method inside async code or if there's an active event loop. Run the agent with a user prompt in async mode, returning a streamed response. Args: user_prompt: User input to start/continue the conversation. result_type: Custom result type to use for this run. usage: Optional usage to start with, useful for resuming a conversation or agents used in tools. infer_name: Whether to try to infer the agent name from the call frame if it's not set. @contextmanager def override ( self , *, deps : AgentDepsT, model : models: models): return None. override_deps_before = self . _override_depS ( deps) # noinspection. yield = True messages = graph_ctx . state . message_history . copy ( streamed_response) if yielded is not None: raise exceptions . AgentRunError ( 'Agent run produced final results' ) on_complete = True Decorator is used to register a system prompt function. It can be used bare (\u2018system_prompt\u2019) or as a function call (\u2019agent.system_ Prompt(...)\u2019). The decorator can also be used to decorate a sync or async function. Decorator to register a result validator function. Can decorate a sync or async functions. Overloads for every possible signature of `result_validator` are included. The docstring is inspected to extract both the tool description and description of each parameter. Decorator to register a tool function which DOES NOT take `RunContext` as an argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter. def tool_decorator ( func_ : ToolFuncPlain [ ToolParams ]) : # noinspection PyTypeChecker self . _register_function ( func, False, retries, prepare, docstring_format , require_parameter_descriptions ) return func. def _get_model ( self , model : models . Model | models . KnownModelName | None ) : \"\"\"Create a model configured for this agent. Args: model: model to use for this run, required if `model` was not set when creating the agent. Returns: The model used\" Infer the agent name from the call frame. If we've overridden deps via `_override_deps', use that, otherwise use the deps passed to the call. This method preserves the generic parameters while narrowing the type, unlike a direct call to 'isinstance' Isinstance is a method that preserves the generic parameters while narrowing the type, unlike a direct call to 'isinstance' The default model configured for this agent is None. If None , we try to infer the agent name from the call frame when the agent is first run. Pydantic's agent.py can be used to create, test, and instrument agents. It can also be used as a tool to register with the agent. There are a number of tools that can be added to the agent, and a strategy for handling tool calls alongside a final result. The name of the tool to use for the final result. Retries: The default number of retries to allow before raising an error. tools: Tools to register with the agent, you can also register tools via the decorators. defer_model_check: by default, if you provide a [named] model, it's evaluated to create a [`Model`]. instance immediately. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an AgentRun object. This is the API to use if you want to consume the outputs coming from each LLM model response. A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an `AgentRun` object. The `Agent run` can beused to access the full message history, new messages, and usage statistics.  model: Optional model to use for this run, required if `model` was not set when creating the agent. deps: Optional dependencies to use in this run. usage: Optional usage to start with, useful for resuming a conversation or agents used in tools. infer_name: Whether to try to infer the agent name from the call frame if it's not set. System.prompt_dynamic_functions = self . _system_prompt-dynamic-functions. graph . iter ( start_node, state = state, deps = graph_deps, infer_name = False, span = use_span ( run_span , end_on_exit = True ), ) as graph_run. yield AgentRun ( graph_ run ) run_sync run_ sync ( user_promPT : str | Sequence [ UserContent ], *, message_history : list [ ModelMessage ] | None = None), usage_limits : UsageLimits | None, usage : Usage | None , infer_ name : bool = True ) -> AgentRunResult [ Synchronously run the agent with a user prompt. Returns: The result of the run. None infer_name: Whether to try to infer the agent name from the call frame if it's not set. usage: Optional usage to start with, useful for resuming a conversation. Run the agent with a user prompt in async mode, returning a streamed response. The result of the run is the response to the question, 'What is the capital of the UK?' The answer is London. Run the agent with a user prompt in async mode, returning a streamed response. Args: user_prompt: User input to start/continue the conversation. result_type: Custom result type to use for this run, `result_type` may only be used if the agent has no result validators. usage: Optional usage to start with, useful for resuming a conversation or agents used in tools. infer_name: Whether to try to infer the agent name from the call frame if it's not set. Usage: Optional limits on model request count or token usage. First, let's start with the first node. The first node should be a user prompt node. Then, we'll try to get a final result from the stream. The final result will be called when the stream has completed.  context manager to temporarily override agent dependencies and model. This is particularly useful when testing. The decorator can be used either bare ( agent.system_prompt ) or as a system prompt function. Decorator can be used either bare (`agent.system_prompt`) or as a function call (` agent.system-prompt(...)), see the examples below. Overloads for every possible signature of system_ Prompt are included so the decorator doesn't obscure the type of function. Decorator to register a result validator function. Optionally takes RunContext as its first argument. Can decorate a sync or async functions. Overloads for every possible signature of result_validator are included. Decorator to register a tool function which takes RunContext as its first argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, learn more. From pydantic_ai import Agent, RunContext agent = Agent('test', deps_type=int) @agent.tool def foobar(ctx: RunContext[int], x: int) -> int: return ctx.deps + x @agent .tool(retries=2) @ agent.tool (retries = 2) #> {\"foobar\":1,\"spam\":1.0} print ( result . data ) #> 'foobar':123, spam:3.14' Pydantic_ai's tool decorator can be used to decorate a sync or async function. The decorator is used to extract both the tool description and description of each parameter. It can't add overloads for every possible signature of tool. Is_model_request_node is a function that checks if a node is a ModelRequest node. Is_call_tools_ node is used to check if a Node is a CallTools node. The UserPrompt node is called if it is a Userprompt node. This method preserves the generic parameters while narrowing the type. A stateful, async-iterable run of an Agent. You generally obtain an AgentRun instance by calling async with my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an End is reached, the run finishes and result becomes available. Pydantic_ai is an open-source Python language. It allows you to run a stateful, async-iterable run of an agent graph. The result is the result of the run if it has ended, otherwise it is empty. The agent class lets you drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [`End`] node. Once the run returns an End node, result is populated with an AgentRun Result. Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [`End`] node.  class AgentRunResult ( Generic [ ResultDataT ]): \"\"\"The final result of an agent run.\"\"\" data : Result dataT # TODO: rename this to output. I'm putting this off for now mostly to reduce the size of the diff. _result_tool_name : str | None = dataclasses . field ( repr = False) _state : _agent_graph . GraphAgentState =dataclasses. field ( rep = False ) _new_message_index : int = datAClasses .Field ( repr=False) def _set_result_ tool_return ( self , return_content : str ) : \"\"\"Set return content for the result tool. Useful if you want to Pydantic_ai.agent.AgentRun Result. run.py is a Python tool that allows you to set the response to the result tool call. The usage function returns the usage of the whole run. The new_messages function returns a list of messages. All_messages returns a list of all the messages associated with a given run. New messages are returned as JSON bytes. Messages from older runs are excluded. The result tool can be used to set the response to the result tool call. Pydantic AI and LLM integrations have two distinct kinds of tests: Unit tests and Evals. Unit tests are tests of your application code, and whether it's behaving correctly. Evals are tests for the LLM, and how good or bad its responses are. Use TestModel or FunctionModel in place of your actual model to avoid the usage, latency and variability of real LLM calls Use Agent.override to replace your model inside your application logic. Set ALLOW_MODEL_REQUESTS=False globally to block any requests from being made to non-test models accidentally. We're using anyio to run async tests. We're using Agent.override to replace the agent's model with TestModel. The nice thing about override is that we can replace the model inside agent without needing access to the agent run* methods call site. We want to test this code without having to mock certain objects. The WeatherService.get_forecast is never called since TestModel calls it with a date in the past. We use FunctionModel to replace the agent's model with our custom function. The IsNow helper allows us to use declarative asserts even with data which will contain timestamps that change over time. \"Evals\" refers to evaluating a models performance for a specific application. The hardest part of evals is measuring how well the model has performed. There are a few different strategies you can use to measure performance. The SqlSystemPrompt class is used to build the system prompt. It can be customised with a list of examples and a database type. We implement this as a separate class passed as a dep to the agent so we can override both the inputs and the logic during evals via dependency injection. We use 5-fold cross-validation to judge the performance of the agent using our existing set of examples. We use Agent.override to replace the system prompt with a custom one that uses a subset of examples, and then run the application code (in this case user_search ). We also run the actual SQL from the examples and compare the \"correct\" result from the example SQL to the SQL generated by the agent. Pydantic is a free, open-source, web-based chat app. It is available on GitHub. Pydantic has been around since the 1970s. It was acquired by Pydant Services Inc. in 2024.  182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 315 316 317 318 319 320 321 322 323 324 325 326 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 7 Pydantic_ai is an open-source, Python-based logging tool. It can be used to test and log OpenTelemetry data. There are a number of built-in features that make it possible to test data in the cloud. Model is the default model to use for this agent, if not provide, you must provide the model when calling it. deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully parameterize the agent. name: The name of the agent, used for logging. result_retries: The maximum number of retries to allow for result validation, defaults to `retries. tools: Tools to register with the agent,. you can also register tools via the decorators. instrument: Automatically instrument with OpenTelemetry if it's configured. end_strategy: Strategy for handling tool calls that are requested alongside a final result. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. This method builds a graph using system prompts and tools. A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and result schemas) and then returns an `AgentRun` object. This is the API to use if you want to consume the outputs coming from each LLM model response. Nodes.append(node) print(nodes) ''' [ ModelRequestNode( request=ModelRequest( parts=[ UserPromptPart( content='What is the capital of France?', timestamp=datetime.datetime(...), part_kind='user-prompt', ) ] ''' print(agent_run.result.data) #> Paris ``` Args: user_prompt: User input to start/continue the conversation. result_type: Custom result type to use for this run. usage: Optional usage to start with, useful for resuming a conversation. infer_name: Whether to try to infer the agent name from the call frame if it's not set. Start_node = _agent_graph . UserPromptNode [ AgentDepsT ]( user_prompt = user.prompts, system_ Prompts = self . _system_Prompts,. system_promPT_dynamic_functions = self. _system-prompt-dynamic-functions, tracer = tracer ) as graph_run : yield AgentRun ( graph_ run ) @overload def run_sync ( self , user_ Promp : str, message_history : list [ _messages . UserContent], usage_limits: None, usage: _usage . UsageLimits): None, infer_name: bool = True , ) -> @overload def run_stream ( self , user_prompt : str | Sequence [ _messages . UserContent ]): ... @asynccontextmanager async def get_event_loop ( self ) : self . run_until_complete ( self . result_type, message_history, model = model, deps = deps, usage_limits = usage_ limits , usage = usage , infer_name = False , ) ) @overloaddef run_ stream ( # noqa C901 self, user_ Prompt: str, result_ type: type [ RunResultDataT], message_ history: list [ _Messages . ModelMessage ], model: models . Model, Run the agent with a user prompt in async mode, returning a streamed response. Example: ```python from pydantic_ai import Agent agent = Agent('openai:gpt-4o') async def main(): with agent.run_stream('What is the capital of the UK?') as response: print(await response.get_data()) #> London. @contextmanager def override ( self , *, deps : AgentDepsT, model : models : models ): override_model_before = self . _override_model # noinspection PyTypeChecker @overload def system_prompt ( self, func : Callable): Callable. message_history . copy () async def on_complete ( messages, parts): pass # TODO: Should we do something here related to the retry count? # Maybe we should move the incrementing of the retries to where we actually make a request? Decorator is used to register a system prompt function. It can be used bare (`agent.system_prompt`) or as a function call. The decorator can also be used to decorate a sync or async function. Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument. Can decorate a sync or async function. Overloads for every possible signature of `result_validator` are included. Python's PyTypeChecker can be used to inspect a tool function's parameters. It can also decorate a sync or async function. The docstring is inspected to extract both the tool description and description of eachparameter. \"Private utility to register a tool instance.\"\"\" if tool . max_retries is None : # noinspection PyTypeChecker tool = dataclasses . replace ( tool , max_ retries = self . _default_retry ) self ._register_tool ( tool ) def _get_model ( self , model : models . Model | models . KnownModelName | None ) : \"\"\"Create a model configured for this agent. Args: model: model to use for this run, required if `model` was not set when creating the agent\"  def last_run_messages ( self): raise AttributeError ( 'The `last_run-messages` attribute has been removed, use `capture_run\u00a0messages' instead) def build_graph ( self, result_type): return Graph ( self . name , self . _deps_type , result_ type or self . result-type) def is_model_request_node ( node : _agent_graph . AgentNode [ T , S ]): return Isinstance ( node , ModelRequest node) @staticmethod def is\u00a0is_user_prompt_ node ( node: Agent node): return TypeGuard ( node . UserPrompt node, result . Final TypeGuard is a way of preserving the generic parameters while narrowing the type. It works like a direct call to `isinstance`. The type of the result data defaults to str . str system_prompt str | Sequence [ str ] Static system prompts to use for this agent. The default number of retries to allow before raising an error. The pydantic_ai_slim/pydantic-ai/agent.py program is used to create an agent for OpenTelemetry. There are a number of built-in tools that can be used to test the agent. Model is evaluated to create a [`Model`] instance immediately, which checks for the necessary environment variables. Set this to `false` to defer the evaluation until the first run. instrument: Automatically instrument with OpenTelemetry. end_strategy: Strategy for handling tool calls that are requested alongside a final result. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. The AgentRun API can be used to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. It also provides methods to access the full message history, new messages, and usage statistics. For more details, see the documentation of AgentRun . The 'AgentRun' function is used to iterate over the nodes of the agent graph. It can be used to consume the outputs coming from each LLM model response. It also provides methods to access the full message history, new messages, and usage statistics. Model. _get_model ( model ) del model deps = self . _ get_deps ( deps ) new_message_index = len ( message_history ) if message_ history else 0 result_schema : _result . ResultSchema [ RunResultDataT ] | None = None = self. _prepare_result_Schema ( result_type) graph. graph ( state = state, usage = usage or _ usage . Usage (), retries = 0, run_step = 0 , ) as graph_run : yield AgentRun ( graph_ run ) run.sync run_sync ( user_prompt : str | Sequence [ UserContent ], * , message_ This is a convenience method that wraps self.run with loop. You therefore can't use this method inside async code or if there's an active event loop. The result of the run is the result of running the agent. Synchronously run the agent with a user prompt. This is a convenience method that wraps [`self.run`][pydantic_ai.Agent.run] with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop. Run the agent with a user prompt in async mode, returning a streamed response. Example: from pydantic_ai import Agent agent = Agent ( 'openai:gpt-4o' ) async def main (): async with agent . run_stream ( 'What is the capital of the UK?' ) as response : print ( await response . get_data ()) #> London.py 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 6 Run the agent with a user prompt in async mode, returning a streamed response. Args: user_prompt: User input to start/continue the conversation. result_type: Custom result type to use for this run. message_history: History of the conversation so far. usage: Optional usage to start with, useful for resuming a conversation. infer_name: Whether to try to infer the agent name from the call frame if it's not set.  isinstance ( maybe_part_event , _messages . PartStartEvent ): new_part = maybe_ part_event . part if _agent_graph . allow_text_result ( result_schema): return FinalResult ( s , None , None ) if yielded is not None: raise exceptions . AgentRunError ( 'Agent run produced final results' ) override override ( *, deps : AgentDepsT | Unset = UNSET , model : Model | KnownModelName | Un Set = UN SET) override override is a context manager to temporarily override agent dependencies and model. The context manager can be used to temporarily override agent dependencies and model. This is particularly useful when testing. Decorator to register a system prompt function. Optionally takes RunContext as its only argument. Decorator can be used either bare (`agent.system_prompt`) or as a function call (\u2018agent. system_ Prompt(...)\u2019), see the examples below. Decorator takes [\u2018RunContext\u2019] as its only argument. Decorator to register a result validator function. Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument. Can decorate a sync or async function. Overloads for every possible signature of `result_validator` are included. Pydantic_ai's tool function decorator can be used to decorate a sync or async function. It takes a function and a docstring as its first argument. The docstring is inspected to extract both the tool description and description of each parameter. Pydantic_ai uses PyTypeChecker to inspect the docstring. The docstring is inspected to extract both the tool description and description of eachparameter. Decorators are used to decorate a sync or async functions. Pydantic_ai is a Python tool that allows you to decorate functions. It can be used to test whether a function is working or not. The tool_plain function lets you decorate a function without taking an argument as an argument. It also lets you check if a node is a ModelRequest node.  is_call_tools_node ( node : AgentNode [ T , S ] | End [ FinalResult [ S ]], ) -> TypeGuard [ CallTools node [ T, S ]] Check if the node is a CallToolsNode, narrowing the type if it is. This method preserves the generic parameters while narrowing thetype, unlike a direct call to isinstance. is_user_prompt_ node ( node: Agent node, end: End, result: Final result) is_end_ nodes ( nodes: Agent nodes, ends: Final results) You generally obtain an AgentRun instance by calling async with my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an End is reached, the run finishes and result becomes available. The code is written in Python and is available on GitHub. It uses the Pydantic_ai.agent.AgentRun.next method to drive the iteration of the agent graph. You can also manually drive the iterations using the [`next`] method. From pydantic_ai, we can import the Agent class. We can then use it to run an agent run. The run will return the next node in the agent graph, or an 'End' node if the run has completed. Advance to the next node automatically based on the last returned node. Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution. Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [`End`] node.  def all_messages ( self , * , result_tool_return_content : str | None = None ) -> list [ _messages . ModelMessage ]: \"\"\"Return the history of the messages associated with this run. Messages from older runs are excluded. This provides a convenient way to modify the content of the result tool call if you want to continue the conversation and want to set the response to the resulttool call. The usage function returns the usage of the whole run. The all_messages function returns a list of all the messages in the current session. The result_tool_return_content function sets the content of the result tool call to the last message. The new_messages function returns new messages associated with this run. Messages from older runs are excluded. The result_tool_return_content function returns the return content of the tool call to set in the last message. PydanticAI is a free, open-source, AI-powered tool for managing agents. It lets agents retrieve extra information to help them generate a response. It is intended to be used in conjunction with RAG (Retrieval-Augmented Generation) Dice game is a pretty simple task, so we can use the fast and cheap Gemini flash model. We pass the user's name as the dependency, to keep things simple we use just the name as a string. This tool doesn't need any context, it just returns a random number. Let's print the messages from that game to see what happened. Using decorators, we can register tools via the tools argument to the Agent constructor. This is useful when you want to reuse tools, and can also give more fine-grained control over the tools. Tools or functions are also used to define the schema(s) for structured responses. Pydantic is a free and open-source Python programming language. It has a number of tools that can be used to create and test applications. The tool definition can be customized at each step of a run. The return type can be anything which Pydantic can serialize to JSON. Here's a simple prepare method that only includes the tool if the value of the dependency is 42 . As with the previous example, we use TestModel to demonstrate the behavior without calling a real model. For the sake of variation, we create this tool using the Tool dataclass."
}